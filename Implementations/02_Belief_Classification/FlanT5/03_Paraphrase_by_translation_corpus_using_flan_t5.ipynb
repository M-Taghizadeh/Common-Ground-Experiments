{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zioVWEmoSLWU"
      },
      "source": [
        "# Install Requirements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6jkH8jee3e1M"
      },
      "outputs": [],
      "source": [
        "!pip install transformers==4.28.1 datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CEcR1RUTBMr5"
      },
      "source": [
        "# Connect to Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n_nxbfHB-XKR",
        "outputId": "1767b1e3-86bc-48eb-fca6-3a309b6a5508"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7jr87bZ-GeJc"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "import glob\n",
        "from datasets import load_dataset\n",
        "from datasets import Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9JXYfgG05DU8"
      },
      "source": [
        "# Load Corpus"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "oeyWDFh7HhS2"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "f = open(\"/content/drive/MyDrive/Corpus/CG_Corpus/cg_3to1_2previous_event_selection.dat\", \"rb\")\n",
        "dataset = pickle.load(f)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M9HIXPRpJKOY",
        "outputId": "ff6bb44c-c9b1-413a-863c-b31fd6db8ac0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['Speaker', 'Sentence_Number', 'Sentence', 'Event', 'Target_Event', 'Bel(A)', 'Bel(B)', 'CG(A)', 'CG(B)'],\n",
              "        num_rows: 970\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['Speaker', 'Sentence_Number', 'Sentence', 'Event', 'Target_Event', 'Bel(A)', 'Bel(B)', 'CG(A)', 'CG(B)'],\n",
              "        num_rows: 325\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzA-Sm3Asf3e"
      },
      "source": [
        "# Paraphrase corpus with translation using flan t5\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qUVl66Yj7CXn"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM, AutoTokenizer\n",
        "\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"google/flan-t5-base\")\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Cm3dnpJ6pYEI"
      },
      "outputs": [],
      "source": [
        "model.to('cuda')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6Q_YCypnuaB"
      },
      "source": [
        "## Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "EI-nI-s9kakn"
      },
      "outputs": [],
      "source": [
        "event = \"\"\"i am a student\"\"\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0mRRgL_qkEBo",
        "outputId": "041f6695-d0e6-4dd7-9d2b-40cc40e047b6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Je suis un étudiant']\n",
            "['Ich bin ein Student']\n",
            "['yo soy un estudiante']\n",
            "['io è un studente']\n"
          ]
        }
      ],
      "source": [
        "inputs = tokenizer(f\"translate English to French: {event}\", return_tensors=\"pt\").to('cuda')\n",
        "outputs = model.generate(**inputs, max_length=256)\n",
        "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
        "\n",
        "inputs = tokenizer(f\"translate English to German: {event}\", return_tensors=\"pt\").to('cuda')\n",
        "outputs = model.generate(**inputs, max_length=256)\n",
        "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
        "\n",
        "inputs = tokenizer(f\"translate English to Spanish: {event}\", return_tensors=\"pt\").to('cuda')\n",
        "outputs = model.generate(**inputs, max_length=256)\n",
        "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))\n",
        "\n",
        "inputs = tokenizer(f\"translate English to Italian: {event}\", return_tensors=\"pt\").to('cuda')\n",
        "outputs = model.generate(**inputs, max_length=256)\n",
        "print(tokenizer.batch_decode(outputs, skip_special_tokens=True))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2bhYPmEFurc8"
      },
      "source": [
        "## Paraphrasing on minority classes (CT-, PS, NB)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ac7Ct_b2zQw"
      },
      "source": [
        "### old prompt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Imj0k0_S21c-",
        "outputId": "1f55a4e8-5d2b-47ca-db73-0f7dfa437a7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paraphrased sentences: 54\n"
          ]
        }
      ],
      "source": [
        "sum_paraphrased = 0\n",
        "for record in dataset['train']:\n",
        "  if record['Bel(A)']==2:\n",
        "    event = str(record['Event']).strip()\n",
        "    input = event\n",
        "\n",
        "    # 1. French\n",
        "    prompt_text = f\"\"\"translate English to French: {input}\"\"\"\n",
        "    inputs = tokenizer.encode_plus(prompt_text, padding='max_length', max_length=512, return_tensors='pt').to('cuda')\n",
        "    outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=512, num_beams=4, early_stopping=True)\n",
        "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    output = prediction\n",
        "    new_record = record.copy()\n",
        "    new_record['Event'] = output\n",
        "    # add paraphrased event\n",
        "    dataset_dict = dataset[\"train\"].to_dict()\n",
        "    for key in new_record: dataset_dict[key].append(new_record[key])\n",
        "    dataset[\"train\"] = Dataset.from_dict(dataset_dict)\n",
        "\n",
        "    # 2. German\n",
        "    prompt_text = f\"\"\"translate English to German: {input}\"\"\"\n",
        "    inputs = tokenizer.encode_plus(prompt_text, padding='max_length', max_length=512, return_tensors='pt').to('cuda')\n",
        "    outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=512, num_beams=4, early_stopping=True)\n",
        "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    output = prediction\n",
        "    new_record = record.copy()\n",
        "    new_record['Event'] = output\n",
        "    # add paraphrased event\n",
        "    dataset_dict = dataset[\"train\"].to_dict()\n",
        "    for key in new_record: dataset_dict[key].append(new_record[key])\n",
        "    dataset[\"train\"] = Dataset.from_dict(dataset_dict)\n",
        "\n",
        "    # 3. Spanish\n",
        "    prompt_text = f\"\"\"translate English to Spanish: {input}\"\"\"\n",
        "    inputs = tokenizer.encode_plus(prompt_text, padding='max_length', max_length=512, return_tensors='pt').to('cuda')\n",
        "    outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=512, num_beams=4, early_stopping=True)\n",
        "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    output = prediction\n",
        "    new_record = record.copy()\n",
        "    new_record['Event'] = output\n",
        "    # add paraphrased event\n",
        "    dataset_dict = dataset[\"train\"].to_dict()\n",
        "    for key in new_record: dataset_dict[key].append(new_record[key])\n",
        "    dataset[\"train\"] = Dataset.from_dict(dataset_dict)\n",
        "\n",
        "    # 4. Italian\n",
        "    # prompt_text = f\"\"\"translate English to Italian: {input}\"\"\"\n",
        "    # inputs = tokenizer.encode_plus(prompt_text, padding='max_length', max_length=256, return_tensors='pt').to('cuda')\n",
        "    # outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=256, num_beams=4, early_stopping=True)\n",
        "    # prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # output = prediction\n",
        "    # new_record = record.copy()\n",
        "    # new_record['Event'] = output\n",
        "    # # add paraphrased event\n",
        "    # dataset_dict = dataset[\"train\"].to_dict()\n",
        "    # for key in new_record: dataset_dict[key].append(new_record[key])\n",
        "    # dataset[\"train\"] = Dataset.from_dict(dataset_dict)\n",
        "\n",
        "    sum_paraphrased += 1\n",
        "\n",
        "print(f\"Paraphrased sentences: {sum_paraphrased}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rywRyD5U22p3",
        "outputId": "ea5a1543-a703-4e32-b1d5-ad7ae3ef4816"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paraphrased sentences: 78\n"
          ]
        }
      ],
      "source": [
        "sum_paraphrased = 0\n",
        "for record in dataset['train']:\n",
        "  if record['Bel(A)']==3:\n",
        "    event = str(record['Event']).strip()\n",
        "    input = event\n",
        "\n",
        "    # 1. French\n",
        "    prompt_text = f\"\"\"translate English to French: {input}\"\"\"\n",
        "    inputs = tokenizer.encode_plus(prompt_text, padding='max_length', max_length=512, return_tensors='pt').to('cuda')\n",
        "    outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=512, num_beams=4, early_stopping=True)\n",
        "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    output = prediction\n",
        "    new_record = record.copy()\n",
        "    new_record['Event'] = output\n",
        "    # add paraphrased event\n",
        "    dataset_dict = dataset[\"train\"].to_dict()\n",
        "    for key in new_record: dataset_dict[key].append(new_record[key])\n",
        "    dataset[\"train\"] = Dataset.from_dict(dataset_dict)\n",
        "\n",
        "    # 2. German\n",
        "    prompt_text = f\"\"\"translate English to German: {input}\"\"\"\n",
        "    inputs = tokenizer.encode_plus(prompt_text, padding='max_length', max_length=512, return_tensors='pt').to('cuda')\n",
        "    outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=512, num_beams=4, early_stopping=True)\n",
        "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    output = prediction\n",
        "    new_record = record.copy()\n",
        "    new_record['Event'] = output\n",
        "    # add paraphrased event\n",
        "    dataset_dict = dataset[\"train\"].to_dict()\n",
        "    for key in new_record: dataset_dict[key].append(new_record[key])\n",
        "    dataset[\"train\"] = Dataset.from_dict(dataset_dict)\n",
        "\n",
        "    # 3. Spanish\n",
        "    prompt_text = f\"\"\"translate English to Spanish: {input}\"\"\"\n",
        "    inputs = tokenizer.encode_plus(prompt_text, padding='max_length', max_length=512, return_tensors='pt').to('cuda')\n",
        "    outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=512, num_beams=4, early_stopping=True)\n",
        "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    output = prediction\n",
        "    new_record = record.copy()\n",
        "    new_record['Event'] = output\n",
        "    # add paraphrased event\n",
        "    dataset_dict = dataset[\"train\"].to_dict()\n",
        "    for key in new_record: dataset_dict[key].append(new_record[key])\n",
        "    dataset[\"train\"] = Dataset.from_dict(dataset_dict)\n",
        "\n",
        "    # 4. Italian\n",
        "    # prompt_text = f\"\"\"translate English to Italian: {input}\"\"\"\n",
        "    # inputs = tokenizer.encode_plus(prompt_text, padding='max_length', max_length=256, return_tensors='pt').to('cuda')\n",
        "    # outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=256, num_beams=4, early_stopping=True)\n",
        "    # prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # output = prediction\n",
        "    # new_record = record.copy()\n",
        "    # new_record['Event'] = output\n",
        "    # # add paraphrased event\n",
        "    # dataset_dict = dataset[\"train\"].to_dict()\n",
        "    # for key in new_record: dataset_dict[key].append(new_record[key])\n",
        "    # dataset[\"train\"] = Dataset.from_dict(dataset_dict)\n",
        "\n",
        "    sum_paraphrased += 1\n",
        "\n",
        "print(f\"Paraphrased sentences: {sum_paraphrased}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "izFG8h7c23Kr",
        "outputId": "028d6df1-e229-4462-d93a-2f70f74ec055"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Paraphrased sentences: 38\n"
          ]
        }
      ],
      "source": [
        "sum_paraphrased = 0\n",
        "for record in dataset['train']:\n",
        "  if record['Bel(A)']==4:\n",
        "    event = str(record['Event']).strip()\n",
        "    input = event\n",
        "\n",
        "    # 1. French\n",
        "    prompt_text = f\"\"\"translate English to French: {input}\"\"\"\n",
        "    inputs = tokenizer.encode_plus(prompt_text, padding='max_length', max_length=512, return_tensors='pt').to('cuda')\n",
        "    outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=512, num_beams=4, early_stopping=True)\n",
        "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    output = prediction\n",
        "    new_record = record.copy()\n",
        "    new_record['Event'] = output\n",
        "    # add paraphrased event\n",
        "    dataset_dict = dataset[\"train\"].to_dict()\n",
        "    for key in new_record: dataset_dict[key].append(new_record[key])\n",
        "    dataset[\"train\"] = Dataset.from_dict(dataset_dict)\n",
        "\n",
        "    # 2. German\n",
        "    prompt_text = f\"\"\"translate English to German: {input}\"\"\"\n",
        "    inputs = tokenizer.encode_plus(prompt_text, padding='max_length', max_length=512, return_tensors='pt').to('cuda')\n",
        "    outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=512, num_beams=4, early_stopping=True)\n",
        "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    output = prediction\n",
        "    new_record = record.copy()\n",
        "    new_record['Event'] = output\n",
        "    # add paraphrased event\n",
        "    dataset_dict = dataset[\"train\"].to_dict()\n",
        "    for key in new_record: dataset_dict[key].append(new_record[key])\n",
        "    dataset[\"train\"] = Dataset.from_dict(dataset_dict)\n",
        "\n",
        "    # 3. Spanish\n",
        "    prompt_text = f\"\"\"translate English to Spanish: {input}\"\"\"\n",
        "    inputs = tokenizer.encode_plus(prompt_text, padding='max_length', max_length=512, return_tensors='pt').to('cuda')\n",
        "    outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=512, num_beams=4, early_stopping=True)\n",
        "    prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    output = prediction\n",
        "    new_record = record.copy()\n",
        "    new_record['Event'] = output\n",
        "    # add paraphrased event\n",
        "    dataset_dict = dataset[\"train\"].to_dict()\n",
        "    for key in new_record: dataset_dict[key].append(new_record[key])\n",
        "    dataset[\"train\"] = Dataset.from_dict(dataset_dict)\n",
        "\n",
        "    # 4. Italian\n",
        "    # prompt_text = f\"\"\"translate English to Italian: {input}\"\"\"\n",
        "    # inputs = tokenizer.encode_plus(prompt_text, padding='max_length', max_length=256, return_tensors='pt').to('cuda')\n",
        "    # outputs = model.generate(inputs['input_ids'], attention_mask=inputs['attention_mask'], max_length=256, num_beams=4, early_stopping=True)\n",
        "    # prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    # output = prediction\n",
        "    # new_record = record.copy()\n",
        "    # new_record['Event'] = output\n",
        "    # # add paraphrased event\n",
        "    # dataset_dict = dataset[\"train\"].to_dict()\n",
        "    # for key in new_record: dataset_dict[key].append(new_record[key])\n",
        "    # dataset[\"train\"] = Dataset.from_dict(dataset_dict)\n",
        "\n",
        "    sum_paraphrased += 1\n",
        "\n",
        "print(f\"Paraphrased sentences: {sum_paraphrased}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HHoidF6pwjDz"
      },
      "source": [
        "# Save Augmented Corpus with Translation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ib48zBtvNuD_",
        "outputId": "480dc379-9579-4b02-bc3d-4b1faf065a9a"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['Speaker', 'Sentence_Number', 'Sentence', 'Event', 'Target_Event', 'Bel(A)', 'Bel(B)', 'CG(A)', 'CG(B)'],\n",
              "        num_rows: 1480\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['Speaker', 'Sentence_Number', 'Sentence', 'Event', 'Target_Event', 'Bel(A)', 'Bel(B)', 'CG(A)', 'CG(B)'],\n",
              "        num_rows: 325\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "R6nSk27CX_Py"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "f = open(\"/content/drive/MyDrive/Corpus/CG_Corpus/cg_3to1_2previous_event_selection_aug.dat\", \"wb\")\n",
        "pickle.dump(dataset, f)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "kk6S5GQuZ7Jr"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "\n",
        "f = open(\"/content/drive/MyDrive/Corpus/CG_Corpus/cg_3to1_2previous_event_selection_aug.dat\", \"rb\")\n",
        "dataset = pickle.load(f)\n",
        "f.close()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TbEjRfRyaAGh",
        "outputId": "7229cd10-ebd5-4850-daca-84096102e745"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['Speaker', 'Sentence_Number', 'Sentence', 'Event', 'Target_Event', 'Bel(A)', 'Bel(B)', 'CG(A)', 'CG(B)'],\n",
              "        num_rows: 1480\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['Speaker', 'Sentence_Number', 'Sentence', 'Event', 'Target_Event', 'Bel(A)', 'Bel(B)', 'CG(A)', 'CG(B)'],\n",
              "        num_rows: 325\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwgzljfeF2D9"
      },
      "outputs": [],
      "source": [
        "# cg_3to1_2previous_event_selection_aug\n",
        "# cg_3to1_previous_speaker_base_event_selection_aug"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}