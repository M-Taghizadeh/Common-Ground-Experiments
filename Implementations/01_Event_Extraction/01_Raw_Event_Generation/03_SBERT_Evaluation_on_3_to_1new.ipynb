{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "! pip install transformers datasets rouge-score sentence_transformers"
      ],
      "metadata": {
        "id": "KNnAp5uw7TTz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# from huggingface_hub import notebook_login\n",
        "\n",
        "# notebook_login()\n",
        "\n",
        "# # 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"
      ],
      "metadata": {
        "id": "gHiq-Hb_bWIO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## What is SBERT Score?\n",
        "\n",
        "https://www.sbert.net/\n",
        "\n",
        "SentenceTransformers is a Python framework for state-of-the-art sentence, text and image embeddings. The initial work is described in our paper Sentence-BERT: Sentence Embeddings using Siamese BERT-Networks.\n",
        "\n",
        "You can use this framework to compute sentence / text embeddings for more than 100 languages. These embeddings can then be compared e.g. with cosine-similarity to find sentences with a similar meaning. This can be useful for semantic textual similar, semantic search, or paraphrase mining.\n",
        "\n",
        "The framework is based on PyTorch and Transformers and offers a large collection of pre-trained models tuned for various tasks. Further, it is easy to fine-tune your own models."
      ],
      "metadata": {
        "id": "Yup8dkCeZInH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dkZc1AYK7O42"
      },
      "outputs": [],
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "\n",
        "sbert_model = SentenceTransformer('all-MiniLM-L6-v2')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Our sentences we like to encode\n",
        "sentences = ['This framework generates embeddings for each input sentence',\n",
        "    'Sentences are passed as a list of string.',\n",
        "    'The quick brown fox jumps over the lazy dog.']\n",
        "\n",
        "# Sentences are encoded by calling model.encode()\n",
        "embeddings = sbert_model.encode(sentences)\n",
        "\n",
        "# Print the embeddings\n",
        "for sentence, embedding in zip(sentences, embeddings):\n",
        "    print(\"Sentence:\", sentence)\n",
        "    print(\"Embedding:\", embedding)\n",
        "    print(\"\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tqOvMuHfZvO0",
        "outputId": "bd17a92e-6b53-4c2d-abc6-075941f6cb9c"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence: This framework generates embeddings for each input sentence\n",
            "Embedding: [-1.37173571e-02 -4.28515747e-02 -1.56286322e-02  1.40537573e-02\n",
            "  3.95537876e-02  1.21796302e-01  2.94333398e-02 -3.17524038e-02\n",
            "  3.54959406e-02 -7.93140233e-02  1.75878070e-02 -4.04369384e-02\n",
            "  4.97259684e-02  2.54912674e-02 -7.18699992e-02  8.14968422e-02\n",
            "  1.47072889e-03  4.79627214e-02 -4.50336076e-02 -9.92174670e-02\n",
            " -2.81769335e-02  6.45046011e-02  4.44670618e-02 -4.76217307e-02\n",
            " -3.52952592e-02  4.38671708e-02 -5.28565943e-02  4.33019857e-04\n",
            "  1.01921454e-01  1.64072476e-02  3.26996669e-02 -3.45986858e-02\n",
            "  1.21339522e-02  7.94871226e-02  4.58342955e-03  1.57778468e-02\n",
            " -9.68206488e-03  2.87626423e-02 -5.05806543e-02 -1.55793941e-02\n",
            " -2.87907012e-02 -9.62280575e-03  3.15556526e-02  2.27349252e-02\n",
            "  8.71449187e-02 -3.85027975e-02 -8.84718895e-02 -8.75499751e-03\n",
            " -2.12343074e-02  2.08924208e-02 -9.02078152e-02 -5.25732376e-02\n",
            " -1.05638430e-02  2.88311411e-02 -1.61455106e-02  6.17842469e-03\n",
            " -1.23234466e-02 -1.07337162e-02  2.83353664e-02 -5.28567620e-02\n",
            " -3.58618237e-02 -5.97989596e-02 -1.09055098e-02  2.91566625e-02\n",
            "  7.97979087e-02 -3.27845191e-04  6.83495542e-03  1.32718366e-02\n",
            " -4.24619764e-02  1.87656749e-02 -9.89234298e-02  2.09050160e-02\n",
            " -8.69605690e-02 -1.50151979e-02 -4.86202240e-02  8.04414600e-02\n",
            " -3.67701310e-03 -6.65044188e-02  1.14556812e-01 -3.04228980e-02\n",
            "  2.96632051e-02 -2.80694719e-02  4.64990661e-02 -2.25514024e-02\n",
            "  8.54223147e-02  3.15446891e-02  7.34542087e-02 -2.21862029e-02\n",
            " -5.29679246e-02  1.27130523e-02 -5.27339578e-02 -1.06188729e-01\n",
            "  7.04731643e-02  2.76737008e-02 -8.05531591e-02  2.39649396e-02\n",
            " -2.65124999e-02 -2.17330754e-02  4.35275473e-02  4.84711863e-02\n",
            " -2.37067025e-02  2.85768453e-02  1.11846149e-01 -6.34936020e-02\n",
            " -1.58318561e-02 -2.26169825e-02 -1.31028118e-02 -1.62068068e-03\n",
            " -3.60929146e-02 -9.78297293e-02 -4.67729084e-02  1.76272113e-02\n",
            " -3.97492349e-02 -1.76444024e-04  3.39627601e-02 -2.09634118e-02\n",
            "  6.33661449e-03 -2.59411372e-02  8.10410678e-02  6.14393353e-02\n",
            " -5.44598605e-03  6.48275986e-02 -1.16844065e-01  2.36861072e-02\n",
            " -1.32058542e-02 -1.12476446e-01  1.90049149e-02 -1.74658463e-34\n",
            "  5.58949634e-02  1.94244757e-02  4.65438738e-02  5.18645830e-02\n",
            "  3.89390290e-02  3.40540707e-02 -4.32114713e-02  7.90637732e-02\n",
            " -9.79530141e-02 -1.27441036e-02 -2.91870572e-02  1.02052307e-02\n",
            "  1.88115984e-02  1.08942538e-01  6.63465038e-02 -5.35295308e-02\n",
            " -3.29229087e-02  4.69827130e-02  2.28882767e-02  2.74114348e-02\n",
            " -2.91983262e-02  3.12706716e-02 -2.22850833e-02 -1.02282166e-01\n",
            " -2.79116444e-02  1.13793118e-02  9.06308964e-02 -4.75414395e-02\n",
            " -1.00718938e-01 -1.23231886e-02 -7.96928406e-02 -1.44636696e-02\n",
            " -7.76400715e-02 -7.66922673e-03  9.73953772e-03  2.24204622e-02\n",
            "  7.77268261e-02 -3.17156967e-03  2.11537816e-02 -3.30394171e-02\n",
            "  9.55250207e-03 -3.73011902e-02  2.61360314e-02 -9.79084242e-03\n",
            " -6.31505325e-02  5.77433920e-03 -3.80031057e-02  1.29684377e-02\n",
            " -1.82499141e-02 -1.56283025e-02 -1.23360497e-03  5.55579476e-02\n",
            "  1.13112692e-04 -5.61256632e-02  7.40165710e-02  1.84451900e-02\n",
            " -2.66368147e-02  1.31951682e-02  7.50087202e-02 -2.46797390e-02\n",
            " -3.24006006e-02 -1.57674812e-02 -8.03515594e-03 -5.61320782e-03\n",
            "  1.05687892e-02  3.26166535e-03 -3.91989872e-02 -9.38677415e-02\n",
            "  1.14227153e-01  6.57304600e-02 -4.72633727e-02  1.45087624e-02\n",
            " -3.54490280e-02 -3.37761454e-02 -5.15506119e-02 -3.81002086e-03\n",
            " -5.15036434e-02 -5.93429506e-02 -1.69410894e-03  7.42107555e-02\n",
            " -4.20091264e-02 -7.19975159e-02  3.17250192e-02 -1.66303217e-02\n",
            "  3.96980764e-03 -6.52750582e-02  2.77391262e-02 -7.51649663e-02\n",
            "  2.27455813e-02 -3.91368307e-02  1.54316146e-02 -5.54908626e-02\n",
            "  1.23318117e-02 -2.59520691e-02  6.66423514e-02 -6.91259922e-34\n",
            "  3.31628770e-02  8.47928971e-02 -6.65584058e-02  3.33541669e-02\n",
            "  4.71610064e-03  1.35362009e-02 -5.38694002e-02  9.20693949e-02\n",
            " -2.96876561e-02  3.16219591e-02 -2.37497371e-02  1.98770780e-02\n",
            "  1.03446215e-01 -9.06947553e-02  6.30628876e-03  1.42886406e-02\n",
            "  1.19293602e-02  6.43727556e-03  4.20104526e-02  1.25344303e-02\n",
            "  3.93019766e-02  5.35691492e-02 -4.30749618e-02  6.10432848e-02\n",
            " -5.39352113e-05  6.91682622e-02  1.05520422e-02  1.22111458e-02\n",
            " -7.23185241e-02  2.50469483e-02 -5.18370792e-02 -4.36562262e-02\n",
            " -6.71818629e-02  1.34827895e-02 -7.25888833e-02  7.04163779e-03\n",
            "  6.58939108e-02  1.08994246e-02 -2.60009500e-03  5.49969077e-02\n",
            "  5.06966598e-02  3.27948444e-02 -6.68833628e-02  6.45557046e-02\n",
            " -2.52076108e-02 -2.92572286e-02 -1.16696730e-01  3.24064232e-02\n",
            "  5.85859008e-02 -3.51756364e-02 -7.15240017e-02  2.24936083e-02\n",
            " -1.00786723e-01 -4.74544875e-02 -7.61962757e-02 -5.87167032e-02\n",
            "  4.21138294e-02 -7.47213960e-02  1.98468175e-02 -3.36503400e-03\n",
            " -5.29736467e-02  2.74729393e-02  3.45736779e-02 -6.11846782e-02\n",
            "  1.06364772e-01 -9.64120030e-02 -4.55945022e-02  1.51489917e-02\n",
            " -5.13531640e-03 -6.64447621e-02  4.31721099e-02 -1.10405814e-02\n",
            " -9.80249606e-03  7.53783360e-02 -1.49571132e-02 -4.80208546e-02\n",
            "  5.80726750e-02 -2.43896935e-02 -2.23138127e-02 -4.36992347e-02\n",
            "  5.12054078e-02 -3.28625962e-02  1.08763345e-01  6.08926490e-02\n",
            "  3.30797047e-03  5.53820021e-02  8.43201205e-02  1.27087366e-02\n",
            "  3.84465419e-02  6.52325898e-02 -2.94683687e-02  5.08005433e-02\n",
            " -2.09348258e-02  1.46135658e-01  2.25561764e-02 -1.77227761e-08\n",
            " -5.02672903e-02 -2.79205153e-04 -1.00328550e-01  2.42811255e-02\n",
            " -7.54043609e-02 -3.79139781e-02  3.96049768e-02  3.10079716e-02\n",
            " -9.05701052e-03 -6.50411919e-02  4.05453034e-02  4.83390167e-02\n",
            " -4.56962399e-02  4.76004463e-03  2.64362688e-03  9.35614482e-02\n",
            " -4.02599312e-02  3.27402428e-02  1.18298279e-02  5.54345362e-02\n",
            "  1.48052230e-01  7.21189529e-02  2.76993145e-04  1.68651026e-02\n",
            "  8.34878534e-03 -8.76156427e-03 -1.33649642e-02  6.14236742e-02\n",
            "  1.57168116e-02  6.94960803e-02  1.08621437e-02  6.08018674e-02\n",
            " -5.33421524e-02 -3.47924456e-02 -3.36272120e-02  6.93907067e-02\n",
            "  1.22987852e-02 -1.45237371e-01 -2.06970912e-03 -4.61132787e-02\n",
            "  3.72750266e-03 -5.59359556e-03 -1.00659855e-01 -4.45953049e-02\n",
            "  5.40921576e-02  4.98895207e-03  1.49534512e-02 -8.26059356e-02\n",
            "  6.26630783e-02 -5.01909386e-03 -4.81857695e-02 -3.53991129e-02\n",
            "  9.03385505e-03 -2.42337491e-02  5.66267259e-02  2.51528826e-02\n",
            " -1.70709137e-02 -1.24779921e-02  3.19518223e-02  1.38420928e-02\n",
            " -1.55815175e-02  1.00178286e-01  1.23657219e-01 -4.22967039e-02]\n",
            "\n",
            "Sentence: Sentences are passed as a list of string.\n",
            "Embedding: [ 5.64524792e-02  5.50023988e-02  3.13795805e-02  3.39484736e-02\n",
            " -3.54247168e-02  8.34667683e-02  9.88800600e-02  7.27545097e-03\n",
            " -6.68657431e-03 -7.65808765e-03  7.93738663e-02  7.39708426e-04\n",
            "  1.49291903e-02 -1.51047045e-02  3.67674455e-02  4.78743240e-02\n",
            " -4.81969751e-02 -3.76052223e-02 -4.60278131e-02 -8.89816284e-02\n",
            "  1.20228186e-01  1.30663291e-01 -3.73936445e-02  2.47855578e-03\n",
            "  2.55820900e-03  7.25814626e-02 -6.80436268e-02 -5.24695776e-02\n",
            "  4.90234382e-02  2.99563147e-02 -5.84429577e-02 -2.02263016e-02\n",
            "  2.08822154e-02  9.76691619e-02  3.52390297e-02  3.91141362e-02\n",
            "  1.05668325e-02  1.56228954e-03 -1.30822891e-02  8.52904469e-03\n",
            " -4.84093605e-03 -2.03766506e-02 -2.71801036e-02  2.83307731e-02\n",
            "  3.66017632e-02  2.51276419e-02 -9.90861654e-02  1.15626613e-02\n",
            " -3.60380560e-02 -7.23784044e-02 -1.12670131e-01  1.12942373e-02\n",
            " -3.86397801e-02  4.67385948e-02 -2.88460609e-02  2.26704106e-02\n",
            " -8.52407515e-03  3.32814977e-02 -1.06584362e-03 -7.09744841e-02\n",
            " -6.31170049e-02 -5.72186746e-02 -6.16026223e-02  5.47146387e-02\n",
            "  1.18317930e-02 -4.66261171e-02  2.56960131e-02 -7.07414187e-03\n",
            " -5.73843382e-02  4.12839167e-02 -5.91503493e-02  5.89021631e-02\n",
            " -4.41697612e-02  4.65081260e-02 -3.15814726e-02  5.58312126e-02\n",
            "  5.54578640e-02 -5.96533343e-02  4.06407379e-02  4.83763404e-03\n",
            " -4.96768244e-02 -1.00944318e-01  3.40078399e-02  4.13273787e-03\n",
            " -2.93529732e-03  2.11837664e-02 -3.73962224e-02 -2.79067252e-02\n",
            " -4.61767539e-02  5.26138693e-02 -2.79734991e-02 -1.62379280e-01\n",
            "  6.61042705e-02  1.72274578e-02 -5.45113767e-03  4.74473946e-02\n",
            " -3.82237434e-02 -3.96896452e-02  1.34544866e-02  4.49653827e-02\n",
            "  4.53674188e-03  2.82978956e-02  8.36633220e-02 -1.00858016e-02\n",
            " -1.19354002e-01 -3.84624563e-02  4.82858792e-02 -9.46083814e-02\n",
            "  1.91854350e-02 -9.96518508e-02 -6.30596876e-02  3.02696005e-02\n",
            "  1.17402291e-02 -4.78372611e-02 -6.20272895e-03 -3.32850739e-02\n",
            " -4.04388085e-03  1.28307398e-02  4.05254997e-02  7.56477043e-02\n",
            "  2.92434767e-02  2.84270272e-02 -2.78938506e-02  1.66857913e-02\n",
            " -2.47961637e-02 -6.83651119e-02  2.89968476e-02 -5.39867674e-33\n",
            " -2.69015669e-03 -2.65069362e-02 -6.47922338e-04 -8.46199226e-03\n",
            " -7.35154897e-02  4.94084042e-03 -5.97842038e-02  1.03438068e-02\n",
            "  2.12898641e-03 -2.88212486e-03 -3.17076743e-02 -9.42364037e-02\n",
            "  3.03019732e-02  7.00227097e-02  4.50685173e-02  3.69439498e-02\n",
            "  1.13593871e-02  3.53027023e-02  5.50449453e-03  1.34413852e-03\n",
            "  3.46122822e-03  7.75048062e-02  5.45112342e-02 -7.92055726e-02\n",
            " -9.31696445e-02 -4.03398462e-02  3.10668889e-02 -3.83081622e-02\n",
            " -5.89442700e-02  1.93331931e-02 -2.67159846e-02 -7.91938379e-02\n",
            "  1.04219951e-04  7.70621300e-02  4.16603610e-02  8.90932679e-02\n",
            "  3.56843397e-02 -1.09153027e-02  3.71498838e-02 -2.07070522e-02\n",
            " -2.46100426e-02 -2.05025263e-02  2.62201596e-02  3.43590267e-02\n",
            "  4.39250879e-02 -8.20515864e-03 -8.40710327e-02  4.24171016e-02\n",
            "  4.87499014e-02  5.95384575e-02  2.87747514e-02  3.37638408e-02\n",
            " -4.07442749e-02 -1.66368368e-03  7.91927427e-02  3.41088548e-02\n",
            " -5.72892546e-04  1.87749658e-02 -1.36964293e-02  7.38333166e-02\n",
            "  5.74465899e-04  8.33505243e-02  5.60810938e-02 -1.13710789e-02\n",
            "  4.42611277e-02  2.69582011e-02 -4.80536111e-02 -3.15087624e-02\n",
            "  7.75226429e-02  1.81773491e-02 -8.83005634e-02 -7.85517227e-03\n",
            " -6.22242913e-02  7.19372854e-02 -2.33474821e-02  6.52483478e-03\n",
            " -9.49530490e-03 -9.88313034e-02  4.01306413e-02  3.07396837e-02\n",
            " -2.21607592e-02 -9.45911184e-02  1.02367932e-02  1.02187827e-01\n",
            " -4.12959494e-02 -3.15777697e-02  4.74752001e-02 -1.10209852e-01\n",
            "  1.69614758e-02 -3.71709503e-02 -1.03261815e-02 -4.72538806e-02\n",
            " -1.20214168e-02 -1.93255283e-02  5.79292402e-02  4.23865585e-34\n",
            "  3.92013118e-02  8.41361657e-02 -1.02946743e-01  6.92259818e-02\n",
            "  1.68821122e-02 -3.26760858e-02  9.65959206e-03  1.80899464e-02\n",
            "  2.17939802e-02  1.63189191e-02 -9.69292447e-02  3.74852330e-03\n",
            " -2.38456856e-02 -3.44055854e-02  7.11962730e-02  9.21922910e-04\n",
            " -6.23862073e-03  3.23753990e-02 -8.90388561e-04  5.01907384e-03\n",
            " -4.24537919e-02  9.89083871e-02 -4.60320711e-02  4.69704643e-02\n",
            " -1.75284222e-02 -7.02518411e-03  1.32744061e-02 -5.30152060e-02\n",
            "  2.66407547e-03  1.45819206e-02  7.43346568e-03 -3.07131652e-02\n",
            " -2.09416524e-02  8.24110210e-02 -5.15894592e-02 -2.71178316e-02\n",
            "  1.17583036e-01  7.72504881e-03 -1.89522952e-02  3.94559801e-02\n",
            "  7.17360601e-02  2.59117186e-02  2.75192149e-02  9.50542651e-03\n",
            " -3.02355420e-02 -4.07944620e-02 -1.04028456e-01 -7.97421858e-03\n",
            " -3.64455534e-03  3.29715759e-02 -2.35954504e-02 -7.50519615e-03\n",
            " -5.82234189e-02 -3.17906402e-02 -4.18049172e-02  2.17453409e-02\n",
            " -6.67292103e-02 -4.89104278e-02  4.58517903e-03 -2.66046599e-02\n",
            " -1.12597018e-01  5.11167236e-02  5.48534095e-02 -6.69856891e-02\n",
            "  1.26766279e-01 -8.59487504e-02 -5.94231710e-02 -2.92189000e-03\n",
            " -1.14875548e-02 -1.26025870e-01 -3.48279579e-03 -9.12001953e-02\n",
            " -1.22933097e-01  1.33777205e-02 -4.75775450e-02 -6.57933131e-02\n",
            " -3.39410082e-02 -3.07107773e-02 -5.22033907e-02 -2.35463586e-02\n",
            "  5.90035319e-02 -3.85757908e-02  3.19700800e-02  4.05118689e-02\n",
            "  1.67077761e-02 -3.58281359e-02  1.45688085e-02  3.20138186e-02\n",
            " -1.34843895e-02  6.07819892e-02 -8.31399858e-03 -1.08105680e-02\n",
            "  4.69410643e-02  7.66134188e-02 -4.23400067e-02 -2.11963336e-08\n",
            " -7.25292861e-02 -4.20227982e-02 -6.12374470e-02  5.24666421e-02\n",
            " -1.42363831e-02  1.18487291e-02 -1.40788676e-02 -3.67530212e-02\n",
            " -4.44977432e-02 -1.15140630e-02  5.23316972e-02  2.96652149e-02\n",
            " -4.62780707e-02 -3.70892957e-02  1.89129785e-02  2.04307772e-02\n",
            " -2.24006101e-02 -1.48563050e-02 -1.79504268e-02  4.20008115e-02\n",
            "  1.40942363e-02 -2.83492561e-02 -1.16862997e-01  1.48956673e-02\n",
            " -7.30581174e-04  5.66028357e-02 -2.68739779e-02  1.09106705e-01\n",
            "  2.94566760e-03  1.19267911e-01  1.14212446e-01  8.92973691e-02\n",
            " -1.70255173e-02 -4.99053970e-02 -2.11931244e-02  3.18421349e-02\n",
            "  7.03435764e-02 -1.02929406e-01  8.23816806e-02  2.81968042e-02\n",
            "  3.21146362e-02  3.79108004e-02 -1.09553121e-01  8.19620490e-02\n",
            "  8.73216614e-02 -5.73563837e-02 -2.01709047e-02 -5.69444075e-02\n",
            " -1.30338678e-02 -5.55684455e-02 -1.32966712e-02  8.64013284e-03\n",
            "  5.30012585e-02 -4.06846963e-02  2.71709133e-02 -2.55946489e-03\n",
            "  3.05775404e-02 -4.61865477e-02  4.68036253e-03 -3.64946947e-02\n",
            "  6.80802390e-02  6.65087551e-02  8.49152282e-02 -3.32849324e-02]\n",
            "\n",
            "Sentence: The quick brown fox jumps over the lazy dog.\n",
            "Embedding: [ 4.39335369e-02  5.89344129e-02  4.81783897e-02  7.75480866e-02\n",
            "  2.67444011e-02 -3.76295969e-02 -2.60506873e-03 -5.99430725e-02\n",
            " -2.49601505e-03  2.20728349e-02  4.80259657e-02  5.57552874e-02\n",
            " -3.89454328e-02 -2.66167913e-02  7.69340945e-03 -2.62376498e-02\n",
            " -3.64160463e-02 -3.78161669e-02  7.40781501e-02 -4.95050251e-02\n",
            " -5.85217029e-02 -6.36196584e-02  3.24350074e-02  2.20085159e-02\n",
            " -7.10637346e-02 -3.31578068e-02 -6.94104061e-02 -5.00374176e-02\n",
            "  7.46268183e-02 -1.11133806e-01 -1.23062972e-02  3.77456434e-02\n",
            " -2.80313529e-02  1.45353470e-02 -3.15585658e-02 -8.05836767e-02\n",
            "  5.83526455e-02  2.59009562e-03  3.92802432e-02  2.57696025e-02\n",
            "  4.98505756e-02 -1.75623747e-03 -4.55297828e-02  2.92608142e-02\n",
            " -1.02017224e-01  5.22287264e-02 -7.90899992e-02 -1.02857659e-02\n",
            "  9.20250639e-03  1.30732507e-02 -4.04777788e-02 -2.77924929e-02\n",
            "  1.24667408e-02  6.72832876e-02  6.81248456e-02 -7.57119339e-03\n",
            " -6.09942432e-03 -4.23776619e-02  5.17815948e-02 -1.56707261e-02\n",
            "  9.56357270e-03  4.12390195e-02  2.14959215e-02  1.04293255e-02\n",
            "  2.73349881e-02  1.87062621e-02 -2.69607436e-02 -7.00542331e-02\n",
            " -1.04700513e-01 -1.89878652e-03  1.77017190e-02 -5.74725680e-02\n",
            " -1.44223105e-02  4.70486615e-04  2.33227410e-03 -2.51920484e-02\n",
            "  4.93004210e-02 -5.09609692e-02  6.31983504e-02  1.49165215e-02\n",
            " -2.70766634e-02 -4.52875644e-02 -4.90594208e-02  3.74940671e-02\n",
            "  3.84579524e-02  1.56900508e-03  3.09922881e-02  2.01630238e-02\n",
            " -1.24363275e-02 -3.06719933e-02 -2.78819390e-02 -6.89182729e-02\n",
            " -5.13677485e-02  2.14795321e-02  1.15747107e-02  1.25408219e-03\n",
            "  1.88765787e-02 -4.42318767e-02 -4.49817926e-02 -3.41866771e-03\n",
            "  1.31131224e-02  2.00099535e-02  1.21099770e-01  2.31074896e-02\n",
            " -2.20160186e-02 -3.28846872e-02 -3.15513322e-03  1.17845564e-04\n",
            "  9.91498753e-02  1.65238865e-02 -4.69669281e-03 -1.45366471e-02\n",
            " -3.71076167e-03  9.65135917e-02  2.85908449e-02  2.13481747e-02\n",
            " -7.17645437e-02 -2.41142288e-02 -4.40940335e-02 -1.07346937e-01\n",
            "  6.79945573e-02  1.30466819e-01 -7.97029510e-02  6.79508969e-03\n",
            " -2.37512123e-02 -4.61637005e-02 -2.99650412e-02 -3.69410083e-33\n",
            "  7.30969831e-02 -2.20171846e-02 -8.61464590e-02 -7.14379773e-02\n",
            " -6.36741370e-02 -7.21863210e-02 -5.93042141e-03 -2.33641788e-02\n",
            " -2.83658486e-02  4.77434732e-02 -8.06176588e-02 -1.56479981e-03\n",
            "  1.38443811e-02 -2.86235772e-02 -3.35386842e-02 -1.13777518e-01\n",
            " -9.17635392e-03 -1.08101303e-02  3.23196314e-02  5.88380694e-02\n",
            "  3.34209092e-02  1.07987940e-01 -3.72713432e-02 -2.96770670e-02\n",
            "  5.17189540e-02 -2.25338656e-02 -6.96090907e-02 -2.14475095e-02\n",
            " -2.33410615e-02  4.82199900e-02 -3.58766653e-02 -4.68990840e-02\n",
            " -3.97873558e-02  1.10813268e-01 -1.43007282e-02 -1.18464492e-01\n",
            "  5.82915395e-02 -6.25889227e-02 -2.94041093e-02  6.03238046e-02\n",
            " -2.44416250e-03  1.60116255e-02  2.67233104e-02  2.49530524e-02\n",
            " -6.49319068e-02 -1.06802611e-02  2.81464383e-02  1.03563173e-02\n",
            " -6.63567625e-04  1.98185910e-02 -3.04288380e-02  6.28420804e-03\n",
            "  5.15268557e-02 -4.75375280e-02 -6.44421577e-02  9.55032334e-02\n",
            "  7.55858198e-02 -2.81574801e-02 -3.49965505e-02  1.01816416e-01\n",
            "  1.98731888e-02 -3.68036851e-02  2.93523166e-03 -5.00745401e-02\n",
            "  1.50932103e-01 -6.16079718e-02 -8.58812928e-02  7.13989744e-03\n",
            " -1.33065376e-02  7.80404657e-02  1.75250489e-02  4.21279334e-02\n",
            "  3.57939675e-02 -1.32950410e-01  3.56970131e-02 -2.03116611e-02\n",
            "  1.24910008e-02 -3.80355157e-02  4.91543189e-02 -1.56540908e-02\n",
            "  1.21418267e-01 -8.08644816e-02 -4.68781367e-02  4.10842933e-02\n",
            " -1.84318125e-02  6.69691414e-02  4.33594221e-03  2.27315035e-02\n",
            " -1.36429472e-02 -4.53238413e-02 -3.92829739e-02 -6.29889313e-03\n",
            "  5.29609658e-02 -3.69065143e-02  7.11677223e-02  2.33343416e-33\n",
            "  1.05231367e-01 -4.81874309e-02  6.95918947e-02  6.56976402e-02\n",
            " -4.65149395e-02  5.14492281e-02 -1.24475369e-02  3.20871845e-02\n",
            " -9.23356786e-02  5.00932857e-02 -3.28876041e-02  1.39139174e-02\n",
            " -8.70206684e-04 -4.90903622e-03  1.03946388e-01  3.21646861e-04\n",
            "  5.28110154e-02 -1.17989983e-02  2.31565293e-02  1.31768370e-02\n",
            " -5.25963120e-02  3.26702148e-02  3.08715011e-04  6.41129017e-02\n",
            "  3.88500951e-02  5.88008426e-02  8.29792991e-02 -1.88149456e-02\n",
            " -2.26377212e-02 -1.00473680e-01 -3.83752212e-02 -5.88081293e-02\n",
            "  1.82419945e-03 -4.26995195e-02  2.50195004e-02  6.40059933e-02\n",
            " -3.77482772e-02 -6.83905091e-03 -2.54603149e-03 -9.76042673e-02\n",
            "  1.88475847e-02 -8.83168133e-04  1.73611660e-02  7.10790604e-02\n",
            "  3.30393203e-02  6.93425816e-03 -5.60523309e-02  5.14634438e-02\n",
            " -4.29542176e-02  4.60077114e-02 -8.78832955e-03  3.17289457e-02\n",
            "  4.93965670e-02  2.95189954e-02 -5.05192317e-02 -5.43187261e-02\n",
            "  1.49999789e-04 -2.76614670e-02  3.46878432e-02 -2.10890174e-02\n",
            "  1.38060162e-02  2.99886726e-02  1.39744971e-02 -4.26471373e-03\n",
            " -1.50336837e-02 -8.76095518e-02 -6.85053840e-02 -4.28141803e-02\n",
            "  7.76944906e-02 -7.10285679e-02 -7.37691950e-03  2.13727541e-02\n",
            "  1.35562029e-02 -7.90464655e-02  5.47666848e-03  8.30663964e-02\n",
            "  1.14148036e-01  1.80764135e-03  8.75491127e-02 -4.16044928e-02\n",
            "  1.55416392e-02 -1.01206638e-02 -7.32437102e-03  1.07965851e-02\n",
            " -6.62816837e-02  3.98413762e-02 -1.16711520e-01  6.42993674e-02\n",
            "  4.02919948e-02 -6.54741228e-02  1.95052177e-02  8.09995756e-02\n",
            "  5.36462963e-02  7.67969713e-02 -1.34852231e-02 -1.76919066e-08\n",
            " -4.43935432e-02  9.20643564e-03 -8.79590362e-02  4.26921323e-02\n",
            "  7.31365159e-02  1.68427788e-02 -4.03263047e-02  1.85131077e-02\n",
            "  8.44172686e-02 -3.74477357e-02  3.02996505e-02  2.90641431e-02\n",
            "  6.36878535e-02  2.89750155e-02 -1.47270150e-02  1.77542828e-02\n",
            " -3.36895026e-02  1.73161328e-02  3.37875374e-02  1.76826090e-01\n",
            " -1.75533034e-02 -6.03078045e-02 -1.43394405e-02 -2.38536336e-02\n",
            " -4.45530489e-02 -2.89850309e-02 -8.96776468e-02 -1.75937358e-03\n",
            " -2.61485931e-02  5.93995396e-03 -5.18355593e-02  8.57279897e-02\n",
            " -8.18398893e-02  8.35439283e-03  4.00790088e-02  4.17764522e-02\n",
            "  1.04573570e-01 -2.86561996e-03  1.96690895e-02  5.81048708e-03\n",
            "  1.33253587e-02  4.51001003e-02 -2.17588246e-02 -1.39492927e-02\n",
            " -6.86992556e-02 -2.94108992e-03 -3.10765002e-02 -1.05854459e-01\n",
            "  6.91623688e-02 -4.24114577e-02 -4.67681885e-02 -3.64751220e-02\n",
            "  4.50400002e-02  6.09816760e-02 -6.56561628e-02 -5.45640849e-03\n",
            " -1.86226927e-02 -6.31484389e-02 -3.87437046e-02  3.46733704e-02\n",
            "  5.55458292e-02  5.21628149e-02  5.61064966e-02  1.02063939e-01]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Usage\n",
        "\n",
        "https://www.sbert.net/docs/usage/semantic_textual_similarity.html"
      ],
      "metadata": {
        "id": "K1pbyP57aA72"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Two lists of sentences\n",
        "sentences1 = ['The cat sits outside',\n",
        "             'A man is playing guitar',\n",
        "             'The new movie is awesome']\n",
        "\n",
        "sentences2 = ['The dog plays in the garden',\n",
        "              'A woman watches TV',\n",
        "              'The new movie is so great']\n",
        "\n",
        "#Compute embedding for both lists\n",
        "embeddings1 = sbert_model.encode(sentences1, convert_to_tensor=True)\n",
        "embeddings2 = sbert_model.encode(sentences2, convert_to_tensor=True)\n",
        "\n",
        "#Compute cosine-similarities\n",
        "cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
        "\n",
        "#Output the pairs with their score\n",
        "for i in range(len(sentences1)):\n",
        "    print(\"{} \\t\\t {} \\t\\t Score: {:.4f}\".format(sentences1[i], sentences2[i], cosine_scores[i][i]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SVBl7jQ9aFGT",
        "outputId": "02ed70f8-261c-4a96-e5d2-9a64a12ce6d5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "The cat sits outside \t\t The dog plays in the garden \t\t Score: 0.2838\n",
            "A man is playing guitar \t\t A woman watches TV \t\t Score: -0.0327\n",
            "The new movie is awesome \t\t The new movie is so great \t\t Score: 0.8939\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "\n",
        "def calculate_sbert_score(sentences1, sentences2):\n",
        "    # Compute embedding for both lists\n",
        "    embeddings1 = sbert_model.encode(sentences1, convert_to_tensor=True)\n",
        "    embeddings2 = sbert_model.encode(sentences2, convert_to_tensor=True)\n",
        "\n",
        "    # ompute cosine-similarities\n",
        "    cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
        "    output = torch.tensor([cosine_scores])\n",
        "    return round(output.item(), 4)"
      ],
      "metadata": {
        "id": "uTh2CzpBZMS2"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import re\n",
        "import glob\n",
        "from datasets import load_dataset\n",
        "import datasets"
      ],
      "metadata": {
        "id": "ppxBXrPxTBRS"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PW9y20lmTCwP",
        "outputId": "0b56e047-1336-43db-f368-30b7549afecd"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pickle\n",
        "\n",
        "f = open(\"/content/drive/MyDrive/Corpus/CG_Corpus/event_extraction_3to1.dat\", \"rb\")\n",
        "dataset = pickle.load(f)\n",
        "f.close()"
      ],
      "metadata": {
        "id": "z0KsvnsrTEMQ"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gg6-w56iTFyK",
        "outputId": "0812635a-0801-4d35-8f3d-37fac5ea7c92"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['Sentence', 'Events'],\n",
              "        num_rows: 415\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['Sentence', 'Events'],\n",
              "        num_rows: 146\n",
              "    })\n",
              "})"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UG80gOlSEon"
      },
      "source": [
        "## Load Pretrained Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NZMuOPt3T8Ev"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "save_directory = \"/content/drive/MyDrive/Common Ground Docs/Models/FlanT5_Event_Extraction_3_to_1\"\n",
        "tokenizer = AutoTokenizer.from_pretrained(save_directory)\n",
        "pretrained_model = AutoModelForSeq2SeqLM.from_pretrained(save_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1y4GNMb7UbyI"
      },
      "outputs": [],
      "source": [
        "pretrained_model.to('cuda')"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Rouge Average"
      ],
      "metadata": {
        "id": "9UvIt29dLJEk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "def calculate_rouge_score(reference, candidate):\n",
        "  scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL', ], use_stemmer=True)\n",
        "  scores = scorer.score(reference, candidate)\n",
        "  return scores['rougeL']"
      ],
      "metadata": {
        "id": "TZusYvH0TJdz"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o3Mqp-88UZDZ",
        "outputId": "0647dff6-a879-4c02-a8b7-5ac40f4baf85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "RougeL average on test set with 146 samples: 0.487449564110134\n"
          ]
        }
      ],
      "source": [
        "samples_number = len(dataset['test'])\n",
        "\n",
        "SUM = 0\n",
        "for sample in dataset['test']:\n",
        "  TEXT = \"Events: \" + sample['Sentence']\n",
        "  ground_truth = sample['Events']\n",
        "  inputs = tokenizer(TEXT, return_tensors=\"pt\").to('cuda')\n",
        "  outputs = pretrained_model.generate(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_length=512)\n",
        "  prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "  rouge = calculate_rouge_score(ground_truth, prediction)\n",
        "  SUM += rouge[2] # rougeL fmeasure\n",
        "\n",
        "rouge_avg = SUM/samples_number\n",
        "print(f\"\\nRougeL average on test set with {samples_number} samples: {rouge_avg}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SBERT Average"
      ],
      "metadata": {
        "id": "vpmUlsUdLK4p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sentence_transformers import SentenceTransformer, util\n",
        "import torch\n",
        "\n",
        "def calculate_sbert_score(sentences1, sentences2):\n",
        "    # Compute embedding for both lists\n",
        "    embeddings1 = sbert_model.encode(sentences1, convert_to_tensor=True)\n",
        "    embeddings2 = sbert_model.encode(sentences2, convert_to_tensor=True)\n",
        "\n",
        "    # ompute cosine-similarities\n",
        "    cosine_scores = util.cos_sim(embeddings1, embeddings2)\n",
        "    output = torch.tensor([cosine_scores])\n",
        "    return round(output.item(), 4)"
      ],
      "metadata": {
        "id": "yV0Ke81WRmB0"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "samples_number = len(dataset['test'])\n",
        "\n",
        "SUM = 0\n",
        "for sample in dataset['test']:\n",
        "  TEXT = \"Events: \" + sample['Sentence']\n",
        "  ground_truth = sample['Events']\n",
        "  inputs = tokenizer(TEXT, return_tensors=\"pt\").to('cuda')\n",
        "  outputs = pretrained_model.generate(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_length=512)\n",
        "  prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "  sbert_score = calculate_sbert_score(ground_truth, prediction)\n",
        "  SUM += sbert_score\n",
        "\n",
        "  if sbert_score<0.2: print(f\"\\n[-] Sentence:{TEXT} \\nground_truth: {ground_truth} \\nprediction: {prediction} \\nsimilarity score: {sbert_score}\", '\\n------------------------------')\n",
        "  if sbert_score>0.9: print(f\"\\n[+] Sentence:{TEXT} \\nground_truth: {ground_truth} \\nprediction: {prediction} \\nsimilarity score: {sbert_score}\", '\\n------------------------------')\n",
        "\n",
        "sbert_score_avg = SUM/samples_number\n",
        "print(f\"\\n\\n\\nSBERT Score Cosine Similarity Average on test set with {samples_number} samples: {sbert_score_avg}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YsQCp7nOLL_X",
        "outputId": "6787869b-e0b2-4899-897a-99e609c6f8ce"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "[+] Sentence:Events: B: yeah.   \n",
            "ground_truth: Event1: B says yeah.  \n",
            " \n",
            "prediction: Event1: B says yeah.  \n",
            "similarity score: 1.0 \n",
            "------------------------------\n",
            "\n",
            "[+] Sentence:Events: A: Darn it. I thought I was going to get to see everybody.   \n",
            "ground_truth: Event1: A thought B was going to get to see everybody\n",
            "Event2: B was going to get to see everybody\n",
            "Event3: B got to see everybody\n",
            " \n",
            "prediction: Event1: A thought B was going to get to see everybody Event2: B was going to get to see everybody  \n",
            "similarity score: 0.9717 \n",
            "------------------------------\n",
            "\n",
            "[+] Sentence:Events: A: What kind of car do you have?  \n",
            "ground_truth: Event1: A asks B what kind of car B has\n",
            "Event2: B has a car\n",
            " \n",
            "prediction: Event1: A asks B what kind of car do A have Event2: A has a car  \n",
            "similarity score: 0.9801 \n",
            "------------------------------\n",
            "\n",
            "[+] Sentence:Events: B: Same car.   \n",
            "ground_truth: Event1: B has the same car\n",
            " \n",
            "prediction: Event1: B says Same car  \n",
            "similarity score: 0.9378 \n",
            "------------------------------\n",
            "\n",
            "[+] Sentence:Events: B: yeah.   \n",
            "ground_truth: Event1: B says yeah.  \n",
            " \n",
            "prediction: Event1: B says yeah.  \n",
            "similarity score: 1.0 \n",
            "------------------------------\n",
            "\n",
            "[+] Sentence:Events: A: What else is going on? You’re busy with work and  \n",
            "ground_truth: Event1: A asks B what else is going on\n",
            "Event2: Something else is going on\n",
            "Event3: B is busy with work\n",
            " \n",
            "prediction: Event1: A asks B what else is going on Event2: B is busy with work  \n",
            "similarity score: 0.9717 \n",
            "------------------------------\n",
            "\n",
            "[+] Sentence:Events: B: yeah. I work ten at night to six in the morning. &Bo &Bo’s happy because %uh he don’t have to go back to day care.   \n",
            "ground_truth: Event1: B works ten at night to six in the morning\n",
            "Event2: Bo's happy because Bo doesn't have to go back to day care\n",
            "Event3: Bo's happy\n",
            "Event4: Bo doesn't have to go back to day care\n",
            " \n",
            "prediction: Event1: B works ten at night to six in the morning Event2: Bo is happy because Bo doesn't have to go back to day care Event3: Bo doesn't have to go back to day care  \n",
            "similarity score: 0.9853 \n",
            "------------------------------\n",
            "\n",
            "[+] Sentence:Events: A: Right. When do you sleep then?  \n",
            "ground_truth: Event1: A asks B when B sleeps then\n",
            "Event2: B sleeps\n",
            " \n",
            "prediction: Event1: A asks B if B sleeps when B wakes up Event2: B sleeps when B wakes up  \n",
            "similarity score: 0.9377 \n",
            "------------------------------\n",
            "\n",
            "[+] Sentence:Events: B: I p- I sleep probably a couple hours in the morning and then like at nap time.   \n",
            "ground_truth: Event1: B sleeps probably a couple hours in the morning\n",
            "Event2: Then B sleeps probably like at nap time\n",
            " \n",
            "prediction: Event1: B sleeps probably a couple hours in the morning Event2: B sleeps like at nap time  \n",
            "similarity score: 0.9932 \n",
            "------------------------------\n",
            "\n",
            "[+] Sentence:Events: A: yeah. You never did require much sleep.   \n",
            "ground_truth: Event1: B never required much sleep\n",
            " \n",
            "prediction: Event1: A never did require much sleep  \n",
            "similarity score: 0.9075 \n",
            "------------------------------\n",
            "\n",
            "[+] Sentence:Events: A: {breath} So has, has %uh the old man and the old lady called you lately?  \n",
            "ground_truth: Event1: A asks B if the old man and the old lady called B lately\n",
            "Event2: The old man and the old lady called B lately\n",
            " \n",
            "prediction: Event1: A asks B if the old man and the old lady called A lately Event2: The old man and the old lady called A lately  \n",
            "similarity score: 0.9787 \n",
            "------------------------------\n",
            "\n",
            "[+] Sentence:Events: A: yeah.   \n",
            "ground_truth: Event1: A says yeah.  \n",
            " \n",
            "prediction: Event1: A says yeah.  \n",
            "similarity score: 1.0 \n",
            "------------------------------\n",
            "\n",
            "[+] Sentence:Events: B: No.   \n",
            "ground_truth: Event1: B says No.  \n",
            " \n",
            "prediction: Event1: B says No.  \n",
            "similarity score: 1.0 \n",
            "------------------------------\n",
            "\n",
            "[+] Sentence:Events: A: And what’s right for &Kara will be right for &David.   \n",
            "ground_truth: Event1: And what's right for Kara will be right for David\n",
            " \n",
            "prediction: Event1: What's right for Kara will be right for David Event2: What's right for David will be right for Kara Event3: David will be right for Kara  \n",
            "similarity score: 0.9032 \n",
            "------------------------------\n",
            "\n",
            "[+] Sentence:Events: A: I wouldn’t I w-, I you know what I could never be with someone who didn’t love me.   \n",
            "ground_truth: Event1: A could never be with someone who didn't love A\n",
            "Event2: Someone didn't love A\n",
            " \n",
            "prediction: Event1: A would not be with someone who didn't love A Event2: A could never be with someone who didn't love A Event3: A could never be with someone who didn't love A  \n",
            "similarity score: 0.9294 \n",
            "------------------------------\n",
            "\n",
            "[+] Sentence:Events: B: Hell no.   \n",
            "ground_truth: Event1: B says Hell no.  \n",
            " \n",
            "prediction: Event1: B says Hell no.  \n",
            "similarity score: 1.0 \n",
            "------------------------------\n",
            "\n",
            "[+] Sentence:Events: A: the %uh [[quietly groaning]] sergeant and lieutenant at the %uh &Des &Moines &McDonald’s  \n",
            "ground_truth: Event1: A has to tell B that A met sergeant and lieutenant at the Des Moines McDonald's\n",
            "Event2: A tells B A met sergeant and lieutenant at the Des Moines McDonald's\n",
            "Event3: A met sergeant and lieutenant at the Des Moines McDonald's\n",
            " \n",
            "prediction: Event1: A asks B if B is a sergeant and lieutenant at the Des Moines McDonald's Event2: B is a sergeant and lieutenant at the Des Moines McDonald's  \n",
            "similarity score: 0.9141 \n",
            "------------------------------\n",
            "\n",
            "[+] Sentence:Events: B: Sergeant and lieutenant?  \n",
            "ground_truth: Event1: B asks A who are sergeant and lieutenant\n",
            " \n",
            "prediction: Event1: B asks A if A is a Sergeant and a Lieutenant Event2: A is a Lieutenant  \n",
            "similarity score: 0.9223 \n",
            "------------------------------\n",
            "\n",
            "[+] Sentence:Events: A: yeah.   \n",
            "ground_truth: Event1: A says yeah.  \n",
            " \n",
            "prediction: Event1: A says yeah.  \n",
            "similarity score: 1.0 \n",
            "------------------------------\n",
            "\n",
            "[+] Sentence:Events: A: {breath} Well then the following weekend was father’s day.   \n",
            "ground_truth: Event1: Then the following weekend was father's day\n",
            " \n",
            "prediction: Event1: The following weekend was father's day  \n",
            "similarity score: 0.9829 \n",
            "------------------------------\n",
            "\n",
            "[+] Sentence:Events: A: And you want to know something?  \n",
            "ground_truth: Event1: A asks B if B wants to know somehting\n",
            "Event2: B wants to know somehting\n",
            " \n",
            "prediction: Event1: A asks B if B wants to know something  \n",
            "similarity score: 0.9014 \n",
            "------------------------------\n",
            "\n",
            "[+] Sentence:Events: B: I know that and I mean i- i- you get blamed, you still get blamed for shit and, and lectured about shit that happens when you’re a kid or, or, or whatever and all that I mean.   \n",
            "ground_truth: Event1: One gets blamed for shit\n",
            "Event2: One gets lectured about shit that happens when one is a kid\n",
            "Event3: Shit happens when one is a kid\n",
            " \n",
            "prediction: Event1: One gets blamed Event2: One still gets blamed for shit Event3: One is lectured about shit that happens when one is a kid Event4: One is lectured about shit that happens when one is a kid  \n",
            "similarity score: 0.9658 \n",
            "------------------------------\n",
            "\n",
            "[+] Sentence:Events: A: [background noise] Because I can’t have that twenty-four hour {breath} bitching and negative. I can’t. I can’t live like that.   \n",
            "ground_truth: Event1: A can't have that twenty-four hour bitching and negative\n",
            "Event2: A can't live with twenty-four hour bitching\n",
            " \n",
            "prediction: Event1: A can't have that twenty-four hour bitching and negative Event2: A can't live like that Event3: A can't live like that  \n",
            "similarity score: 0.9167 \n",
            "------------------------------\n",
            "\n",
            "[+] Sentence:Events: B: mhm.   \n",
            "ground_truth: Event1: B says mhm.  \n",
            " \n",
            "prediction: Event1: B says mhm.  \n",
            "similarity score: 1.0 \n",
            "------------------------------\n",
            "\n",
            "\n",
            "\n",
            "SBERT Score Cosine Similarity Average on test set with 146 samples: 0.6996178082191783\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Create CSV File for Event Extraction Model"
      ],
      "metadata": {
        "id": "ctmQNP-tTO0x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "events_dict = {\n",
        "    'Input Sentence': [],\n",
        "    'Ground Truth Events': [],\n",
        "    'FlanT5 Generated Events': [],\n",
        "    'SBERT Similarity Score': [],\n",
        "    'rouge1': [],\n",
        "    'rouge2': [],\n",
        "    'rougeL': [],\n",
        "    'rougeLsum': [],\n",
        "    'Difflib Similarity Score': [],\n",
        "}"
      ],
      "metadata": {
        "id": "vettvQt8TgYm"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from rouge_score import rouge_scorer\n",
        "\n",
        "def calculate_rouge_score(reference, candidate):\n",
        "  scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL', 'rougeLsum'], use_stemmer=True)\n",
        "  scores = scorer.score(reference, candidate)\n",
        "  return scores"
      ],
      "metadata": {
        "id": "3uPqMcrOVYWi"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import difflib\n",
        "\n",
        "def cal_similarity_ratio(target_text, memory):\n",
        "  similarity = difflib.SequenceMatcher(None, target_text, memory).ratio()\n",
        "  return similarity"
      ],
      "metadata": {
        "id": "1iciaQx6BIjN"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for sample in dataset['test']:\n",
        "  TEXT = \"Events: \" + sample['Sentence']\n",
        "  ground_truth = sample['Events']\n",
        "  inputs = tokenizer(TEXT, return_tensors=\"pt\").to('cuda')\n",
        "  outputs = pretrained_model.generate(input_ids=inputs[\"input_ids\"], attention_mask=inputs[\"attention_mask\"], max_length=512)\n",
        "  prediction = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "  sbert_score = calculate_sbert_score(ground_truth, prediction)\n",
        "  rouge1 = calculate_rouge_score(ground_truth, prediction)['rouge1'][2]\n",
        "  rouge2 = calculate_rouge_score(ground_truth, prediction)['rouge2'][2]\n",
        "  rougeL = calculate_rouge_score(ground_truth, prediction)['rougeL'][2]\n",
        "  rougeLsum = calculate_rouge_score(ground_truth, prediction)['rougeLsum'][2]\n",
        "  difflib_score = cal_similarity_ratio(ground_truth, prediction)\n",
        "\n",
        "  events_dict['Input Sentence'].append(TEXT)\n",
        "  events_dict['Ground Truth Events'].append(ground_truth)\n",
        "  events_dict['FlanT5 Generated Events'].append(prediction)\n",
        "  events_dict['SBERT Similarity Score'].append(sbert_score)\n",
        "  events_dict['rouge1'].append(rouge1)\n",
        "  events_dict['rouge2'].append(rouge2)\n",
        "  events_dict['rougeL'].append(rougeL)\n",
        "  events_dict['rougeLsum'].append(rougeLsum)\n",
        "  events_dict['Difflib Similarity Score'].append(difflib_score)"
      ],
      "metadata": {
        "id": "Iq1ch0oKTOL0"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "events_df = pd.DataFrame.from_dict(events_dict)"
      ],
      "metadata": {
        "id": "5eMwiDLZV2jv"
      },
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "events_df"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 641
        },
        "id": "kkG3QN-OYn3e",
        "outputId": "94f65ef7-4d02-4423-ce9f-04c16f849ce1"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                                        Input Sentence  \\\n",
              "0    Events: B: %um I took them to %uh &Jill’s and ...   \n",
              "1               Events: A: You mean at te- in &Texas?    \n",
              "2                                   Events: B: yeah.     \n",
              "3    Events: B: And their mom and dad drove down th...   \n",
              "4    Events: A: So how are they getting back? Drivi...   \n",
              "..                                                 ...   \n",
              "141  Events: A: Dad [distortion] You know what I sa...   \n",
              "142                                  Events: B: What?    \n",
              "143  Events: A: I said I hope that my, my sons neve...   \n",
              "144  Events: B: True. I don’t think my kids will be...   \n",
              "145  Events: A: And he just looked at me. [channel ...   \n",
              "\n",
              "                                   Ground Truth Events  \\\n",
              "0    Event1: B took the kids to Jill's\\nEvent2: The...   \n",
              "1    Event1: A asks B if B took the kids to Texas\\n...   \n",
              "2                             Event1: B says yeah.  \\n   \n",
              "3    Event1: The kids' mom and dad drove down to Te...   \n",
              "4    Event1: A asks B how are the kids getting back...   \n",
              "..                                                 ...   \n",
              "141  Event1: A asks B if B knows what A said to A a...   \n",
              "142  Event1: B doesn't know what A said to A and B'...   \n",
              "143  Event1: A said A hopes that A's sons never tre...   \n",
              "144  Event1: B doesn't think B's kids will be like ...   \n",
              "145           Event1: A and B's dad just looked at A\\n   \n",
              "\n",
              "                               FlanT5 Generated Events  \\\n",
              "0    Event1: B took the baby and the babysitter to ...   \n",
              "1    Event1: A asks B if B means at Te- in Texas Ev...   \n",
              "2                                Event1: B says yeah.    \n",
              "3    Event1: B's friends's mom and dad drove down t...   \n",
              "4    Event1: A asks B how the nurses are getting ba...   \n",
              "..                                                 ...   \n",
              "141  Event1: A said to dad when A's dad met A at th...   \n",
              "142  Event1: B asks A what is the baby's name Event...   \n",
              "143  Event1: A said A hopes that A's, her sons neve...   \n",
              "144  Event1: B doesn't think B's kids will be that ...   \n",
              "145  Event1: The lawyer just looked at A Event2: Th...   \n",
              "\n",
              "     SBERT Similarity Score    rouge1    rouge2    rougeL  rougeLsum  \\\n",
              "0                    0.8440  0.515464  0.336842  0.515464   0.494845   \n",
              "1                    0.7602  0.555556  0.411765  0.555556   0.555556   \n",
              "2                    1.0000  1.000000  1.000000  1.000000   1.000000   \n",
              "3                    0.7047  0.622222  0.418605  0.577778   0.577778   \n",
              "4                    0.6928  0.622222  0.372093  0.577778   0.622222   \n",
              "..                      ...       ...       ...       ...        ...   \n",
              "141                  0.8231  0.515837  0.310502  0.461538   0.470588   \n",
              "142                  0.6105  0.317460  0.032787  0.222222   0.222222   \n",
              "143                  0.8681  0.516854  0.436782  0.516854   0.494382   \n",
              "144                  0.8470  0.454545  0.428571  0.454545   0.454545   \n",
              "145                  0.6545  0.521739  0.285714  0.434783   0.434783   \n",
              "\n",
              "     Difflib Similarity Score  \n",
              "0                    0.371257  \n",
              "1                    0.675325  \n",
              "2                    0.954545  \n",
              "3                    0.719626  \n",
              "4                    0.368852  \n",
              "..                        ...  \n",
              "141                  0.085202  \n",
              "142                  0.308300  \n",
              "143                  0.595420  \n",
              "144                  0.541176  \n",
              "145                  0.509434  \n",
              "\n",
              "[146 rows x 9 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-be61d2eb-8a07-4565-9c06-ccffe2e0aec2\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Input Sentence</th>\n",
              "      <th>Ground Truth Events</th>\n",
              "      <th>FlanT5 Generated Events</th>\n",
              "      <th>SBERT Similarity Score</th>\n",
              "      <th>rouge1</th>\n",
              "      <th>rouge2</th>\n",
              "      <th>rougeL</th>\n",
              "      <th>rougeLsum</th>\n",
              "      <th>Difflib Similarity Score</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>Events: B: %um I took them to %uh &amp;Jill’s and ...</td>\n",
              "      <td>Event1: B took the kids to Jill's\\nEvent2: The...</td>\n",
              "      <td>Event1: B took the baby and the babysitter to ...</td>\n",
              "      <td>0.8440</td>\n",
              "      <td>0.515464</td>\n",
              "      <td>0.336842</td>\n",
              "      <td>0.515464</td>\n",
              "      <td>0.494845</td>\n",
              "      <td>0.371257</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>Events: A: You mean at te- in &amp;Texas?</td>\n",
              "      <td>Event1: A asks B if B took the kids to Texas\\n...</td>\n",
              "      <td>Event1: A asks B if B means at Te- in Texas Ev...</td>\n",
              "      <td>0.7602</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.411765</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.555556</td>\n",
              "      <td>0.675325</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>Events: B: yeah.</td>\n",
              "      <td>Event1: B says yeah.  \\n</td>\n",
              "      <td>Event1: B says yeah.</td>\n",
              "      <td>1.0000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>0.954545</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>Events: B: And their mom and dad drove down th...</td>\n",
              "      <td>Event1: The kids' mom and dad drove down to Te...</td>\n",
              "      <td>Event1: B's friends's mom and dad drove down t...</td>\n",
              "      <td>0.7047</td>\n",
              "      <td>0.622222</td>\n",
              "      <td>0.418605</td>\n",
              "      <td>0.577778</td>\n",
              "      <td>0.577778</td>\n",
              "      <td>0.719626</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>Events: A: So how are they getting back? Drivi...</td>\n",
              "      <td>Event1: A asks B how are the kids getting back...</td>\n",
              "      <td>Event1: A asks B how the nurses are getting ba...</td>\n",
              "      <td>0.6928</td>\n",
              "      <td>0.622222</td>\n",
              "      <td>0.372093</td>\n",
              "      <td>0.577778</td>\n",
              "      <td>0.622222</td>\n",
              "      <td>0.368852</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>141</th>\n",
              "      <td>Events: A: Dad [distortion] You know what I sa...</td>\n",
              "      <td>Event1: A asks B if B knows what A said to A a...</td>\n",
              "      <td>Event1: A said to dad when A's dad met A at th...</td>\n",
              "      <td>0.8231</td>\n",
              "      <td>0.515837</td>\n",
              "      <td>0.310502</td>\n",
              "      <td>0.461538</td>\n",
              "      <td>0.470588</td>\n",
              "      <td>0.085202</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>142</th>\n",
              "      <td>Events: B: What?</td>\n",
              "      <td>Event1: B doesn't know what A said to A and B'...</td>\n",
              "      <td>Event1: B asks A what is the baby's name Event...</td>\n",
              "      <td>0.6105</td>\n",
              "      <td>0.317460</td>\n",
              "      <td>0.032787</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.222222</td>\n",
              "      <td>0.308300</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>143</th>\n",
              "      <td>Events: A: I said I hope that my, my sons neve...</td>\n",
              "      <td>Event1: A said A hopes that A's sons never tre...</td>\n",
              "      <td>Event1: A said A hopes that A's, her sons neve...</td>\n",
              "      <td>0.8681</td>\n",
              "      <td>0.516854</td>\n",
              "      <td>0.436782</td>\n",
              "      <td>0.516854</td>\n",
              "      <td>0.494382</td>\n",
              "      <td>0.595420</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>144</th>\n",
              "      <td>Events: B: True. I don’t think my kids will be...</td>\n",
              "      <td>Event1: B doesn't think B's kids will be like ...</td>\n",
              "      <td>Event1: B doesn't think B's kids will be that ...</td>\n",
              "      <td>0.8470</td>\n",
              "      <td>0.454545</td>\n",
              "      <td>0.428571</td>\n",
              "      <td>0.454545</td>\n",
              "      <td>0.454545</td>\n",
              "      <td>0.541176</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>145</th>\n",
              "      <td>Events: A: And he just looked at me. [channel ...</td>\n",
              "      <td>Event1: A and B's dad just looked at A\\n</td>\n",
              "      <td>Event1: The lawyer just looked at A Event2: Th...</td>\n",
              "      <td>0.6545</td>\n",
              "      <td>0.521739</td>\n",
              "      <td>0.285714</td>\n",
              "      <td>0.434783</td>\n",
              "      <td>0.434783</td>\n",
              "      <td>0.509434</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>146 rows × 9 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-be61d2eb-8a07-4565-9c06-ccffe2e0aec2')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-be61d2eb-8a07-4565-9c06-ccffe2e0aec2 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-be61d2eb-8a07-4565-9c06-ccffe2e0aec2');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "events_df.to_csv('Event_Extraction_Results_3_to_1.csv')"
      ],
      "metadata": {
        "id": "UnQEwbMrYprG"
      },
      "execution_count": 23,
      "outputs": []
    }
  ]
}